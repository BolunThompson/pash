%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[sigplan,10pt,review,anonymous]{acmart}
\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[sigplan]{acmart}\settopmatter{}

\acmSubmissionID{248}

\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{xspace}
\usepackage{color}
\usepackage{xcolor}
\usepackage{upquote}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{wrapfig}

\captionsetup[figure]{font=footnotesize,name={Fig.},labelfont={bf, footnotesize}}
\captionsetup[table]{font=footnotesize,name={Tab.},labelfont={bf, footnotesize}, skip=2pt, aboveskip=2pt}
\captionsetup{font=footnotesize,labelfont={bf, footnotesize}, belowskip=2pt}

\newcommand{\eg}{{\em e.g.}, }
\newcommand{\ie}{{\em i.e.}, }
\newcommand{\etc}{{\em etc.}\xspace}
\newcommand{\vs}{{\em vs.} }
\newcommand{\cmpn}{compartmentalization} 
\newcommand{\heading}[1]{\vspace{4pt}\noindent\textbf{#1}\enspace}
\newcommand{\ttt}[1]{\texttt{\small #1}}
\newcommand{\ttiny}[1]{\texttt{\scriptsize #1}}
\newcommand{\tti}[1]{\texttt{\scriptsize #1}}
\newcommand{\spol}[1]{\scriptsize{\sc#1}}
\newcommand{\pol}[1]{\texttt{\small {\color{purple}#1}}}
\newcommand{\rf}[1]{\ref{#1}}
\newcommand{\wka}{\ttt{a\textsubscript{1}}}
\newcommand{\wkq}{\ttt{q\textsubscript{1-4}}}

\newcommand{\cn}[1]{\mbox{\textcircled{\footnotesize #1}}}
\newcommand{\tcn}[1]{\mbox{\textcircled{\scriptsize #1}}}

\newcommand{\pur}{\cn{\textsc{P}}\xspace}
\newcommand{\sta}{\cn{\textsc{S}}\xspace}
\newcommand{\dfs}{\cn{\textsc{F}}\xspace}
\newcommand{\sid}{\cn{\textsc{E}}\xspace}
\newcommand{\irr}{\cn{\textsc{I}}\xspace}

\newcommand{\tpur}{\tcn{\textsc{P}}\xspace}
\newcommand{\tsta}{\tcn{\textsc{S}}\xspace}
\newcommand{\tdfs}{\tcn{\textsc{F}}\xspace}
\newcommand{\tsid}{\tcn{\textsc{E}}\xspace}
\newcommand{\tirr}{\tcn{\textsc{I}}\xspace}

% For comments
\newcommand{\eat}[1]{}
\newcommand{\TODO}[1]{\hl{\textbf{TODO:} #1}\xspace}
\newcommand{\todo}[1]{\hl{#1}\xspace}
\newcommand{\nv}[1]{[{\color{cyan}#1 --- Nikos}]}
\newcommand{\kk}[1]{[{\color{magenta}#1 --- kk}]}
\newcommand{\review}[1]{{\color{red}#1}}

\definecolor{editorGray}{rgb}{0.95, 0.95, 0.95}
\definecolor{editorOcher}{rgb}{1, 0.5, 0} % #FF7F00 -> rgb(239, 169, 0)
\definecolor{editorGreen}{rgb}{0, 0.5, 0} % #007C00 -> rgb(0, 124, 0)

\definecolor{cdb}{rgb}{0.37, 0.62, 0.63} % cadet blue

\lstdefinelanguage{sh}{
  morekeywords={for, in, do, done, \|},
  keywordstyle=\color{purple}\ttfamily,
  % ndkeywords={curl, grep, wget, awk, xargs, find, nc, mdc, gunzip, cut, sort, head, join},
  ndkeywordstyle=\color{black}\ttfamily\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{\#},
  commentstyle=\color{lightgray},
% morecomment=[s]{/\\*\\*, \\*/},
  stringstyle=\color{darkgray}\ttfamily,
  morestring=[b]',
  morestring=[b]",
% numbersep=1pt,
% numberstyle=\footnotesize\bf\color{gray},   % the style that is used for the line-numbers
  abovecaptionskip=0pt,
  aboveskip=0pt,
  belowcaptionskip=0pt,
  belowskip=0pt,
  frame=none                     % adds a frame around the code
% moredelim=[s][\color{gray}]{c:}{>},
% moredelim=[s][\color{orange}]{/*}{/}
}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\small\ttfamily,  % the size of the fonts that are used for the code
  upquote=true,
  captionpos=b,                    % sets the caption-position to bottom
% frame=B,                    % adds a frame around the code
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=2pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{gray},   % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  framerule=0pt,
	xleftmargin=0pt,
	xrightmargin=0pt,
	breakindent=0pt,
  aboveskip=0pt,
  framesep=0pt,
  abovecaptionskip=0pt,
  aboveskip=0pt,
  belowcaptionskip=0pt,
  belowskip=0pt,
  frame=none,
  framexbottommargin=0pt,
  resetmargins=true
}



%% Conference information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmConference[PL'18]{ACM SIGPLAN Conference on Programming Languages}{January 01--03, 2018}{New York, NY, USA}
\acmYear{2018}
\acmISBN{} % \acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%\citestyle{acmauthoryear}  %% For author/year citations
%\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from traditional SIGPLAN
%% proceedings format to PACMPL format must update the
%% '\documentclass' and topmatter commands above; see
%% 'acmart-pacmpl-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption


\begin{document}

%% Title information
\title{Dish: Distribution-oblivious Shell Scripting}         %% [Short Title] is optional;
% \titlenote{with title note}             %% \titlenote is optional;
%                                         %% can be repeated if necessary;
%                                         %% contents suppressed with 'anonymous'
% \subtitle{Subtitle}                     %% \subtitle is optional
% \subtitlenote{with subtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}                    %% \country is recommended
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}                   %% \country is recommended
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}                   %% \country is recommended
}
\email{first2.last2@inst2b.org}         %% \email is recommended

\newcommand{\cf}[1]{(\emph{Cf}.\S\ref{#1})}
\newcommand{\sx}[1]{(\S\ref{#1})}
\newcommand{\sys}{{\scshape Dish}\xspace}
\newcommand{\unix}{{\scshape Unix}\xspace}

\setlist{noitemsep,leftmargin=10pt,topsep=2pt,parsep=2pt,partopsep=2pt}

%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
  Distributed systems offer notable benefits over centralized ones.
  Reaping these benefits, however, requires programming in a way that makes distribution explicit---\eg via new programming languages, distributed frameworks, or source-level annotations.
  This paper presents \sys, a shell variant that automatically and correctly scales out distribution-oblivious shell pipelines. 
  \sys's insight is that shell pipelines already express stream computations that can be automatically distributed.
  To achieve this, \sys
decomposes primitives into distributability classes,
identifies high-distributability stages,
applies rewriting rules for largest possible subprograms,
and orchestrates the distributed program.
Its runtime component provides orchestration and planning support during the execution of the program.
Experiments for complex pipelines show substantial speedups and the ability to operate on large input datasets, all without any developer input.
\end{abstract}

% These are from the abstract:
%  with regards to the sequential program.
  % leveraging the insight that such programs already express stream computations, with their stages falling under a few, known distributability classes.
% a series of techniques for automatically 
% 
% Key challenges include 
% surprisingly expressive, 
% maximally distributable subprograms
% 


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
% \keywords{keyword1, keyword2, keyword3}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}
\label{intro}

% 1. Shell scripting; pipelines, in particular, are a common abstraction for expressing filters
% They are an easy way to , because they combine a set of assumptions that work well with Unix
% They work great on a single machine, but are difficult to scale out
% Could we fully automate distribution? 
% The key insight is that pipelines already express a domain-specific
% computation that is easily amenable to distribution.
% 

Distributed systems offer significant benefits over their centralized counterparts---for example, they can speed up expensive computations or can process data that would not fit into any single machine.
% offer notable benefits over their centralized counterparts:
%   using partitioning and replication, they can process and store data with increased throughput and fault-tolerance.
% Yet, only a minority of developers, employed by the select few companies that deal with massive datasets, have the luxury of engineering software systems with distribution baked in from the start.
% The remaining majority starts by developing and deploying software in a centralized manner---that is, \emph{until} there is a significant change of requirements, such as a load increase.
Despite these benefits, their development remains different from and significantly more difficult than their centralized counterparts.
Whereas anyone can quickly stitch together a Bash script to compute on a single computer, 
  % domain-experts routinely glue scripts together to process and share data. % without the help of a computing expert.
   scaling out to multiple ones requires expert labor around ``point'' solutions with expensive setups, restricted programming interfaces, and exorbitant composition costs~\cite{taurus:14, dios:13, andromeda:15, pywren:17, futuredata:18, nefele:18}.

To understand this sharp contrast, consider a Bash pipeline calculating term frequencies over a set of inputs:

% \begin{lstlisting}[language=sh,float=h,numbers=none]
% cat doc.ms |                   # print input file
% groff -t -e -mandoc -Tascii |  # remove formatting
% col -bx |                      # remove backspaces 
% tr A-Z a-z |                   # convert to lower case
% tr -d '[:punct:]' |            # remove punctuation
% sort |                         # put words in alphabetical order
% unique |                       # remove duplicate words
% comm -13 /usr/share/dict -     # report words not in dictionary 
% \end{lstlisting}

\begin{lstlisting}[language=sh, float=h, numbers=none, escapeinside={($}{$)}]
cat * | tr -cs A-Za-z\n | tr A-Z a-z |    ($$[p_1]$$)
  sort | uniq -c | sort -rn | head 5 > out
\end{lstlisting}

\noindent
Program $p_1$ creates a character stream, breaks it into words, transliterates to lower-case, sorts to group duplicates, reduces duplicates to a counter, sorts counters in descending order, picks the top five results, and writes them to a file \ttt{out}.
Combining several features, \unix makes small tasks easy to express;
  for a user with \emph{one} computer and small input size, this pipeline takes a few seconds to compose and execute~\cite{bentley1986literate}.
% composition allows the composition of general primitives

% Key insight 
% % The garden-hose philosophy is similar to Haskell 
% This pipeline takes an input file (line 1), removes 
% * do not need to use specialized frameworks for composition---instead, as long as they conform to the shell, individual primitives can be written in any programming language.
% * do not need to rewrite ---

\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth]{./figs/dish_schematic.pdf}
\caption{
  \textbf{\sys high-level schematic.}
	\sys XYZ
}
\label{fig:schematic}
\end{figure}


%      dish      ignis, mozart    hadoop, spark, ...      erlang, elixir
%       \/           \   /             \     /                \   /
% auto <-------------------------------------------------------------> manual
%                      [rewrite (not write) cost]

Unfortunately, developing $p_1$'s distributed equivalent requires a significant effort.
% For a user with many computers and larger inputs, 
% The scope of such rewrites, and therefore the cost of manual effort, can vary considerably.
For simple pipelines, fitting into restricted models of computation, this effort amounts to expressing the computation using the primitives provided by a big-data framework~\cite{mapreduce:08, ciel:11, spark:12, naiad:13} or domain-specific language~\cite{alvaro2011consistency, distal:13, meiklejohn2015lasp}---an unjustifiable cost for one-off pipelines that take a few minutes to compose (but are applied to large datasets).
More complex pipelines, such as the ones presented in later sections, would involve a full-fledged distributed programming language~\cite{erlang:96, lopes1997d, acute:05, mace:07, cloudhaskell:11, ScalaLoci:18}. %  or a distributed operating system---\eg Plan9's \ttt{rc} shell.
In both cases, manual rewriting is expensive and can introduce new bugs, cascading changes, and divergence from legacy functionality.
Could the generation and execution of $p_1$'s distributed version be fully and correctly automated?

% Some (parts of) pipelines require only moderate effort, as they are 
% For some pipelines, this effort is merely moderate, as 
%  can be expressed in domain-specific frameworks---for example, Hadoop and Spark provide a few purely functional primitives that 
%   ---still requiring rewriting the program in a different (\eg Scala for Spark).
% 
% This effort is moderate if the program expressed by these pipelines fits into a domain-specific framework.
% At times, such pipelines
% As long as the program fits into the model this is only moderately difficult;
% 
% For many legacy pipelines  
%  has to choose between three options, all of which require significant manual effort.
% The most invasive is to rewrite the entire program in a distributed programming language
% The most popular is to leverage 
% 
% Use a distributed programming language, even higher rewrites
% Or use some form of annotations 
% 
%  this requires rewriting portions of the program in a new language and use operations
% 
% 
% In the simplest case, annotations
% 
% Often, they only focus on a few parts of the system---for example, upgrading to a distributed storage layer.
% More rarely, companies rewrite entire systems (\eg Twitter's Ruby-to-Scala rewrite~\cite{twitter}), a process that is notoriously difficult under schedule constraints and competitive pressures~\cite{rewrite1, rewrite2}.
%  especially since software today makes extensive use of third-party modules~\cite{breakapp:plos:2017}.
% 

% If developers have already expressed their computation as a Unix pipeline, they should not have to manually rewrite the program in other environment---\eg Hadoop, Spark---to exploit distribution.


To answer this question, this paper presents \sys, a shell \todo{preprocessor} that transforms shell pipelines into their distributed equivalents enabling \emph{distribution-oblivious programming}.
The key insight behind \sys is that the language of the Unix shell already encodes stream processing, providing most of the information required to distribute a computation.
\sys builds on this insight with a careful study of the distributability properties of shell primitives and commands, a pipeline rewriting pass that identifies maximal sub-expressions as candidates for scale-out, and a late-bound command-prefixing scheme for deferring scheduling decisions to the distributed planner (\ie when runtime information is available).
A \sys-enabled $p_1$ will run \ttt{tr} and \ttt{uniq} in completely parallel streams, use a mostly-parallel tree of \ttt{sort}s, run \ttt{head} in one of the streams, and (trivially) merge streams in \ttt{out}.

The combination of automated transformations for program distribution with the ability to maintain correct non-distributed semantics results in several benefits.
% \sys converts legacy shell pipelines into their distributed equivalents fully automatically, offering $100\times$ improvements in performance without a single line of additional code or annotation.
First, shell users composing pipelines (or simply running legacy pipelines on massive datasets) can see scalability benefits without any manual effort---no need for \ttt{qsub}~\cite{gentzsch2001sun}, \textsc{SLURM}~\cite{yoo2003slurm}, calls to \textsc{GNU} \ttt{parallel}~\cite{Tange2011a}, or any manual rewriting~\cite{mapreduce:08, ciel:11, spark:12}.
Second, developers of new shell commands can use a concise a domain-specific language to express their distributability properties rather than using ad-hoc, command-specific flags such as {\tt -t},  {\tt NUM\_THREADS}, \ttt{-t}, \ttt{-p} \etc
Most importantly, \sys provides an architectural lesson for system designers---namely, that large-scale research efforts in the distributed- and operating-system literature to provide a \unix-like distributed equivalent~\cite{ousterhout1988sprite, mullender1990amoeba, pike1990plan9, barak1998mosix} would have been simplified by a thin (but sophisticated) rewriting shim like the one \sys provides.
% Nice critique on parallel: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=597050#75

% % Make Unix benefits explicit?
% Indeed, the primary reason behind $p_1$'s succinctness is that the pipeline is a domain-specific language for describing operations over streams.
% Key elements of \unix are the ability to compose programs written in different languages, the abstraction of a file system as a set of resident streams, a small but extensible library of commands, and the ability to resolve names within a global context.
% Under the hood, the \unix kernel buffers results, synchronizes processing stages, and generally orchestrates the computation.

% Primitives 
% \sys enables \emph{distribution-oblivious programming}: 
%   multi-order of magnitude speedups with correctness guarantees and without any developer effort.
% 
% The a series of transformations that 
% Second, we are using describing composition . For this, we are restricting the study to a subset of the POSIX shell--
% 
% Local operations and identifiers have to be carefully translated to distributed ones, a translation that depends on the types of the operations and associated identifiers.
% 
% The semantics is enough to capture powerful features such as stream branching and feedback constructs found in practice and in the system's evaluation.

The paper is structured as follows.
It first starts with a background section outlining pipeline concepts and overviewing \sys~\sx{bg}.
Sections \ref{classes}--\ref{recipes} highlight key contributions:\footnote{
  This is not precise, we'll need to wait until it matures a bit.
}
\begin{itemize}

  \item
  \S\ref{classes} overviews \sys and introduces several scalability and distributability classes.

  \item
  \S\ref{rewrite} presents a set of rewriting transformations for identifying maximal pipeline sub-expressions, candidates for scale-out.

  \item
  \S\ref{other} details other concerns, such as distributed state management and extensibility.
\end{itemize}

\noindent
\sys's evaluation~\sx{eval} shows significant speedups on both parallel and distributed execution, using a combination of micro-bench\-marks---popular shell one-liners that highlight certain features---and multi-line macro-benchmarks---a web crawling and indexing engine, a genomics pipeline, and a popular weather analysis script.
After a comparison with related prior work~\sx{related}, the paper closes with a discussion of related work and possible future directions~\sx{discussion}.

% Additional material, in anonymized form, is available at~\href{https://git.io/Je6Nk}{https://git.io/Je6Nk}.

% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.49\textwidth]{./figs/dish_overview.pdf}
% \caption{
%   \textbf{Applying \sys to $p_1$.}
% }
% \label{fig:example}
% \end{figure*}
% 

\section{Background}
\label{bg}

This section breaks down $p_1$ to presents important background on and challenges
of shell scripting.

\subsection{Unix Composition, Informally}
\label{bg:pipelines}

% A pipeline is a mechanism for program composition that works by chaining together programs (or, \emph{commands}) using pipes.
\unix provides several ways for succinct program composition.
Central among them is the \emph{pipe}, a primitive that passes the output of one program as input to the next.
These programs produce output and consume input concurrently, and possibly at different rates, with the \unix kernel facilitating program scheduling and communication synchronization behind the scenes.
Intermediate programs, termed commands, can also be developed in different programming languages, as long as they conform to a common interface.
\begin{wrapfigure}{r}{0.06\textwidth}
  \vspace{-12pt}
  % \begin{center}
    % \includegraphics[width=0.48\textwidth]{gull}
    \includegraphics[width=0.06\textwidth]{./figs/dish_ex1.pdf}
  % \end{center}
  \vspace{-30pt}
\end{wrapfigure}
This interface exposes a stream of contiguous character lines.
A few characters are special---for example, the newline (\textsc{NL}) character delineates an element of the stream, and the end-of-file (\textsc{EOF}) character signals the end of a stream.
As part of this interface, each command has access to (any combination of) three \emph{standard} streams---input, output, and error.
% the input stream (stdin), the output stream (stdout), and the generating errors or diagnostics to the standard error stream (stderr)
% generating errors or diagnostics to the standard 
% Leveraging this interface, pipelines chain together commands by their standard streams, such that the output stream of one command (stdout) is passed directly as input (stdin) to the next one.

Commands can be tuned by two mechanisms, namely command-line options and environment variables.
Options control the execution of a program---\eg changing the \ttt{sort}ing order.
The shell does not have any visibility into these options; 
  their parsing and evaluation is left entirely up to each command.
Environment variables carry more general information about the surrounding system---\eg file search paths, system defaults, parsing switches \etc 
They are organized in a map from names to values, both of which are strings, and can be evaluated at any point in the program---including as commands, flags, a shell feature termed parameter \emph{expansion}.

% TODO: Talk about security primitives?
Streams are a key feature of \unix, and go hand-in-hand with the file abstraction:
  files are simply persistent streams that can be manipulated by commands or pipelines.
\begin{wrapfigure}{r}{0.15\textwidth}
  \vspace{-15pt}
  % \begin{center}
    % \includegraphics[width=0.48\textwidth]{gull}
    \includegraphics[width=0.15\textwidth]{./figs/dish_ex3.pdf}
  % \end{center}
  \vspace{-25pt}
\end{wrapfigure}
Reading a file or a set of files generates a stream; 
  similarly, streams can be trivially redirected to the file system---by appending with the file identifier.
Files are named, in the sense that they correspond to a global identifier---for example \ttt{cat /x/y} returns the persistent stream associated with the identifier \ttt{/x/y}.
\unix exposes a few different types of files---\eg directories, symbolic links, named pipes (or FIFOs), and special character devices.
As a result, file-system identifiers part of a pipeline may resolve to in-memory structures or pseudo-devices (\eg \ttt{/dev/urandom}).

% At a high level, POSIX defines several file types:
% (-) regular files, storing lines of text or binary data,
% (d) directories, grouping together multiple other files,
% (l) symbolic links, pointing to other files,
% (p) named pipes or FIFOs, communication primitives that can be named and manipulated like files,
% (s) domain sockets or DSes, full-duplex communication primitives that support passing file descriptors, and
% (b or c) special files, interfaces to device drivers or low-level abstractions, presented as ordinary files.

Special files are particularly interesting because they are resolved to OS structures.
One example is the special \ttt{/proc} filesystem (procfs) that presents information about processes and the operating system, offering a feature analogous to programmatic introspection:
  with \ttt{procfs} commands can inspect and alter their directory or the flags passed to them.
% Here are a few examples:
% (i) \ttt{/proc/PID/cmdline} contains the command that originally started the process,
% (ii) \ttt{/proc/modules} contains a list of the kernel modules currently loaded, and
% (iii) \ttt{/proc/PID/cwd} contains a symlink to the current working directory of the process.
Similarly, \unix exposes file-system handles for various ephemeral structures, including standard streams.
This bidirectional relationship between ephemeral streams and persistent names---the fact that streams can be named in the file-system and files can be converted to streams---is a key enabler of the runtime rebinding achieved during our transformations.

A small but powerful set of features is concerned with stream manipulation.
Some of these features are built into the shell;
  for example, given a file identifier, stream redirection operators can direct stdout or stderr to a file.
\begin{wrapfigure}{r}{0.15\textwidth}
  \vspace{-15pt}
  % \begin{center}
    % \includegraphics[width=0.48\textwidth]{gull}
    \includegraphics[width=0.15\textwidth]{./figs/dish_ex3.pdf}
  % \end{center}
  \vspace{-25pt}
\end{wrapfigure}
As described before, the file can be a FIFO dubbing as input to another command.
More interestingly, some commands allow splitting (resp. merging) a single stream (resp. many streams) into multiple streams (resp. a single stream);
  stream merging can be achieved by appending one stream after the end of another or by fusing (zipping) streams column-wise into a single (fat) stream.
% TODO: talk about \ttt{xargs}?

% Explicit Redirection, Implicit splitting and merging, This can be done through the file-system operators 

The combination of file-system identifiers pointing to ephemeral streams and the stream redirection described above enable surprisingly expressive pipelines. %---for example:
The most interesting feature of these pipelines is the presence of cycles:
  a pipeline segment processes some input, part of which is extracting the next input to process.
One example use case, discussed in the evaluation section~\sx{eval} is a web crawler that feeds extracted URLs from the input stream back to the start of the pipeline.
% These streams are created by the C library (glibc) at the start of program execution,
% and new streams can be created to connect to files, sockets, pipes,

Apart from pipes, the \unix shell provides several other forms of program composition such as sequential (\ttt{;}) and parallel (\ttt{\&}) composition operators.
Command substitution, expressed as \ttt{\$($c$)}, replaces the expression with the results of executing $c$.
Code blocks, expressed as \ttt{\{$c_1; c_2$\}}, can redirect streams for the entire group of $c_1$ and $c_2$;
  this is analogous to an anonymous function, which contains many statements and can possibly be assigned to a variable that can later be invoked with parameters.
Subshells, enclosed in \ttt{($c_1; c_2$)}, are similar to code blocks but spawn a child shell, thus avoiding the pollution of their environment variable context. % but incurring higher overhead.


% \begin{lstlisting}[
%   language=es,
%   mathescape,
%   float=t,
%   belowskip=-5mm,
%   label=core-pl,
%   upquote=true,
%   caption={
%     \textbf{The core language of the POSIX shell.}
%     The language used by ; file identifiers and expansion are not shown.
%   }]
% $p$ ::= $s\in{}String$ | $n\in{}Number$ | $b\in{}Bool$ | $\emptyset$
% $v$ ::= $p$ | ($x$,$\ldots$) => {e} | {$s$:$v$,$\ldots$} | [$v$,$\ldots$]
% $e$ ::= x | $v$ | ($x$ = $e$) $e$ | $e$($e$) | $e$[$e$] = $e$
% \end{lstlisting}

% \begin{lstlisting}[language=sh, float=h, numbers=none, escapeinside={($}{$)}]
%  cat /proc/ >                            (($$p_4$$))
% \end{lstlisting}
% \begin{lstlisting}[language=sh, float=h, numbers=none, escapeinside={($}{$)}]
%  ls -l | wc -l                           (($$p_2$$))
% \end{lstlisting}
% \begin{lstlisting}[language=sh, float=h, numbers=none, escapeinside={($}{$)}]
%  cat * | wc -l > out.txt                 (($$p_3$$))
% \end{lstlisting}
% \begin{lstlisting}[language=sh, float=h, numbers=none, escapeinside={($}{$)}]
%  cat /proc/ >                            (($$p_5$$))
% \end{lstlisting}
% \begin{lstlisting}[language=sh, float=h, numbers=none, escapeinside={($}{$)}]
% { echo 1; echo 2; } > out.txt
% \end{lstlisting}

\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth]{./figs/dish_overview.pdf}
\caption{
  \textbf{\sys transformation overview.}
	\sys XYZ
}
\label{fig:overview}
\end{figure}


% \heading{Generation}
% Expansion---the key is that expansion and evaluation are handled by the shell at an earlier stage 
% The most powerful---and interesting, for \ttt{dish}---type of expansion is subshell expansion, that allows replacing entire input streams 

\subsection{Overview}

As outlined earlier, the key insight behind \sys is that the shell already describes parallel computations over streams.
Intuitively,
 (i) the parallel-composition operator exposes task parallelism,
 (ii) commands such as \ttt{xargs} and \ttt{tee} expose data parallelism,
 (iii) the pipe operator enables both task and data parallelism, and
 (iv)  other composition operators can be viewed as barrier candidates.
To leverage these insights and correctly distribute a pipeline such as $p_1$, \sys must solve several challenges.

Understanding the shell's primitives is not enough;
  \sys has to understand the distribution characteristics of individual commands used to compose larger programs.
This challenge can be broken down into two parts.
First, there is a need to understand the commands already in the shell---that is, built-ins that come with any shell and support its core functions.
One example is the set of commands defined by the \textsc{POSIX} standard~\cite{}.
\sys addresses this part by a careful study of a group of commands, identifying a small but clear set of well-understood distributability classes.
This set is already useful enough to allow the composition of practical pipelines~\sx{eval}.

The second part is expressing such characteristics extensions that fall outside the built-in set, allowing new commands to leverage \sys's power.
Commands are arbitrary programs, but still conform to the shell's pipeline interface.
To solve this part, \sys leverages the prior study to define a small but expressive domain-specific language (DSL) for describing a command's key properties with respect to its distributability.
Using this DSL, developers of commands (not users of the shell) can quickly and easily capture the class of the command.
A command developer can only annotate the default behavior;
  if a new flag is added a few years later, and if that flag alters the command's class, only then a new annotation is needed.

Given a set of commands composed together by the shell primitives, \sys's next challenge is to analyze it and identify candidate subexpressions for distribution.
\sys addresses this challenge with a rewriting \todo{pass} that performs dataflow analysis identifying maximal distributability sub-expressions.
It converts the AST to a dataflow graph, and runs...
This analysis is not trivial, as many of the shell's dynamic features have to be taken into consideration---examples include shell expansion, environment variables, file input sizes, \etc

Finally, given a distributed pipeline with runtime \todo{annotations}, \sys needs to schedule and execute it.
This amounts to solving a few challenges, achieved by \sys's runtime component, two of which stand out.
First, \sys needs to wrap single-node commands with corrective runtime interposition based on their classes;
  this is achieved by thin wrappers for all the commands involved in the program, generated just-in-time using the distributability specification outlined earlier.
The reasoning behind wrappers being thin is to not interfere with legacy functionality---commands have many flags accumulated over long periods of time.

Second, \sys needs to fill-in planing details left blank by the rewriting pass.
This is achieved with \emph{planner activators}, commands that yield (and pass their arguments) to the scheduler.
From the point of view of the shell, activators are higher-order commands \'{a} la \ttt{xargs} and \ttt{time} that take other commands as arguments---a technique that achieves the latest possible binding, when critical runtime information is available.

The next few sections discuss the details.
They also outline other, less obvious challenges---such as the synchronization of environment variables, distributed file operation, \etc.


\section{Distributability Classes}
\label{distributability}

To have any hope of distributing \unix pipelines, \sys first needs to understand the distribution characteristics of their intermediate commands.
This understanding is encoded in a command's \emph{distributability} class.
Informally, a distributability class represents the level of synchronization required by copies of a command executing in parallel.
It focuses on characteristics that are important for distribution, rather than all the observable behavior of a command.
\sys leans towards a few coarse classes rather than many detailed ones---among other reasons, to simplify their use by command developers.
% Make the connections that this is like a type, that doesn't capture everything, but is good-enough

This section starts by defining these classes, paired with a distributability study of the commands found in standard libraries such as {\sc POSIX} and GNU Coreutils~\sx{cmd}.
Building on this study, it presents a DSL that enables command classification by its developers~\sx{ext}.
In return, \sys uses this DSL to tag commands in the standard libraries and generate their wrappers presented in later sections.

\subsection{Distributability of Standard Libraries}
\label{cmd}

% nv TODO: this is a hierarchy
Broadly, shell commands can be split into five major classes (summarized in Tab.~\ref{tab:classes}) with respect to their distribution characteristics, depending on how they interact with state.
These classes are ordered in ascending difficulty of distribution:
  later classes require more effort.
In this order, each class can be thought as a subset of the next one---\eg all stateless commands are pure---meaning that the synchronization mechanisms required for any superclass would work with its subclass (without seeing any performance improvements).

\begin{table}[t]
\center
\footnotesize
\setlength\tabcolsep{3pt}
\caption{
  \footnotesize{
    \textbf{Distributability Classes}.
    Broadly, \unix commands can be broken down into to five classes.
  }
}
\begin{tabular}{l @{\extracolsep{\fill}} llll}
\toprule
Class                           &  Key    & Example Commands                            & Coreutils       & POSIX       \\
\midrule
Stateless                       & ~\tsta  & \tti{tr},   \tti{cat},    \tti{grep}        &  22 (21.1\%)     &           \\  % 22
Pure                            & ~\tpur  & \tti{sort}, \tti{wc},     \tti{uniq}        &  21 (20.1\%)     &           \\  % 21
(Distr.) File-System~           & ~\tdfs  & \tti{cp},   \tti{ln},     \tti{rm}          &  27 (25.9\%)     &           \\  % 27
Side-effectful                  & ~\tsid  & \tti{env},  \tti{chroot}, \tti{whoami}      &  25 (24\%)       &           \\  % 25
Irreversible                    & ~\tirr  & \tti{exit}, \tti{lpr},    \tti{reboot}      &  5  (0.4\%)      &           \\  % 5
% \midrule
% Shell PL Constructs             &                         &                  \\
% \etc                            &                         &                  \\
\bottomrule
\end{tabular}
\label{tab:classes}
\end{table}


% \heading{Preliminaries}
% \heading{POSIX, GNU Core-utils, and beyond}

\heading{Stateless Commands}
The first class contains stateless commands~\sta that operate on individual elements of the stream, without maintaining state across invocations.
These are commands that can be expressed as a purely functional map---\eg \ttt{basename} removes a path prefix from a string and \ttt{grep} filters out individual lines.
At times, stateless commands may operate as functions that produce multiple elements---\eg when they insert a {\sc NL} token.
Workloads that use only stateless commands are trivial to parallelize:
  they do not require any synchronization to maintain correctness, nor caution about where to split.
 % map :: (a -> b) -> [a] -> [b]

Many of these commands (about 1/3 of coreutils' \sta) are stateless \emph{within} an element (\eg \ttt{tr} transliterates characters, one at a time).
This property could allow further parallelization within a single line element, a feature that might seem of limited use because these commands are not particularly expensive computationally precisely due to their narrow focus.
However, it is useful for cases with very large stream elements (long lines) such the \ttt{.fastq} format used in bioinformatics pipelines.

\heading{Pure Commands}
The second class contains pure commands \pur.
These commands respect functional purity, returning the same results for the same inputs, but maintain internal state across their entire pass.
The details of this state and its propagation during piecewise element processing affects their distributability characteristics.

Some commands are easy to parallelize, because they maintain trivial state and are commutative---\eg \ttt{wc}, which simply maintains a counter.
Others, such as \ttt{sha1sum}, maintain more complex state that is not applied commutatively.

Often these commands do not operate in an online fashion, but need to block until the end of a stream---a typical example of this is \ttt{sort}, which cannot start before the last line of input has been consumed.
Non-streaming constraints affect task parallelism, but not data parallelism:
  \ttt{sort} can be parallelized significantly using divide-and-conquer techniques.

\heading{File-system Commands}
The third class contains commands that access the file-system~\dfs.
This is the largest class, as the file (and file-system) abstraction is central to the \unix design and philosophy~\cite{unix}.
In fact, \unix uses the file-system as a proxy to several file-unrelated operations---\eg access control and device driving is grafted upon the file-system.

% TODO: mention that some commands fail to comply with the interface---for example, csplit could be split followed by a redirection to files.
Commands in \dfs are distributable for many reasons, as evidenced by a long history of distributed \unix file-system clones~\cite{catalogue}.
Some of these offer the semantics of the \unix file-system---hierarchies, protection bits---and a POSIX virtualization layer that enables support for the commands in \dfs.
The vast majority of these commands are control-plane operations that act on identifiers~\sx{bg}---for example, \ttt{touch} for creating a file or \ttt{chgrp} for changing group ownership.
% these operate by changing an inode
Data-plane operations on files go explicitly through stream-redirection operations~\sx{bg}.
Operations using different identifiers pose no problems;
  operations sharing a single identifier might have problems---but these risks exist even in the non-distributed case.
While such pathological cases are possible, they are rare enough---and have been rare enough since \unix's inception~\cite[\S3.6]{unix}).

% \unix itself does not offer any guarantees when a file is accessed concurrently on a single computer.

\heading{Side-effectful Commands}
Another large class of commands are ones that have side-effects to memory~\sid.
Because of timesharing and multi-tasking---\ie the fact that many users can operate multiple terminals on single \unix machine---a subset of these commands are devoted to accessing and sharing memory such as environment variables and shell parsing flags.

A command in \sid will require some form of concurrency control to distribute property.
However, because its side-effects stay within the distributed \unix environment, they can be manipulated using---including optimistic and multi-versioning schemes that can roll-back side-effects.

The vast majority of these commands (a ratio of 21:4) only \emph{read} values, and usually values that are never affected by the user.
For example, \ttt{date}, \ttt{uname}, and \ttt{finger} are all commands that interface with kernel- or hardware-generated information that is not writable by user-space scripts.
Even if \sid commands are protected with transactions
  these benefits are even greater for read-only or read-mostly accesses.

\heading{Irreversibly Side-effectful Commands}
Finally, some commands have side-effects outside \sys---for example, by printing a file with \ttt{lpr} or restarting with \ttt{reboot}.
Such commands are not distributable---they have to refer to a single location---and have to be guarded with pessimistic transaction control mechanism.

\section{Extensibility}
\label{ext}

Here is the type for \ttt{bwa}, a command that performs a Burrows-Wheeler transform over genomic data:
% ( bwa ) :: ( Pure )  => { In \/ File* } -> { Out /\ Err }    -- default case
%    | -h :: ( Sless ) => { Out }                              -- if needed
\begin{lstlisting}[language=sh, float=h, numbers=none, escapeinside={($}{$)}]
(bwa) :: (Pure) ($$\Rightarrow$$){In($$\lor$$)File*}($$\rightarrow$$){Out($$\land$$)Err}
  |-h :: (Sless)($$\Rightarrow$$){Out}                              
\end{lstlisting}

It states that the command defaults to the Pure class.
It's operation can be thought as a transformation from input streams to output streams.
It operates either on stdin or, if this doesn't exist, one or more files specified as arguments;
% TODO: check actual man page for how to specify files
  it writes to the output and error stream rather than a file.
The \ttt{-h} flag moves \ttt{bwa} into the Stateless class;
  its output is a constant function.


ULTRA DRAFTY:

\kk{Text point: A problem that is specific to the shell is
  extensibility.}

The major characteristic that makes the shell popular and very useful
is its extensibility, as also mentioned in \Cref{bg}. Commands change,
get extended, and each user continuously installs more commands to use
in their system.

\kk{Text point: This restricts the space of solutions for distribution
  of shell scripts}

This means that any one-time solution is not satisfactory, since it
would very easily become obsolete, or it would only handle a limited
amount of shell scripts.

\kk{Text point: The characteristics of a good solution}

A solution that applies to shell programms should thus be extensible,
in the sense that adding or changing a command, doesn't invalidate the
assumptions that the tool makes. In our case, the information about
the command categories. A developer of a new command, should be able
to categorize it accordingly, and succinctly describe how the system
can distribute a command.

\kk{Solution: A lightweight language that can be used by a command
  developer to categorize it for distribution.}

We address this issue by designing a lightweight language whose
purpose is to categorize a command, describes how it reads its input
(meaning in what order), and how to distribute it (in case it is
pure). \kk{I am not sure if the order in which a command reads its
  input will be part of the language since many commands can be made
  to read from one place (like stdin or their first argument). Even in
  that case though, where the command only reads from one argument, we
  have to make sure that there is a way for the user to pinpoint which
  argument does a command read from.}

\subsection{Categorization Language}

\kk{Text point: Introduce problem: Most shell commands, belong to
  different classes depending on their flags, arguments}

Consider the command \texttt{cat} \kk{Maybe that is not he best
  example. TODO: Find a good example of a simple command that is in
  different classes depending on the arguments.}. When used without
any flag it is stateless, etc...

\kk{Text point: Our language must be able to categorize commands to
  different classes according to their arguments.}

\kk{We should have a crisp point about why we have this categorization
  language, and why don't we just allow someone to write a function in
  python for each command, that given the command and its arguments,
  returns whether the command category. Possible arguments include,
  the fact that this language is simpler to use, especially by non
  experts that just run some script but have installed some commands
  that are not ``supported''. Another possible argument is that we
  could be able to reason about the constructs of the categorization
  lagnauge, and that they will be easier to read and
  understand. Another argument is that since these categorizations
  should be shareable among users, it would be bad to execute
  arbitrary python code, so using this language, expressivity is
  limited. All of these arguments are a little bit weak though. Niko,
  what do you think?}

Sketch of the language:

Some kind of lightweight annotation configuration language (like
yaml), that contains a record for each command.

A record contains a sequence of predicates => categories. Each
predicate is a combination of not, or, and and the atomic predicates
are the presence of a flag, or an equality that checks that the value
of a flag is equal to something (because flags can also have
values). The final predicate can be left empty, and it is trivially true.

Interpreter:

Given a command and the set of its arguments, the intepreter returns
the command category. In does so by returning the category of the first
predicate that matches.

\kk{Would it be meaningful to give the syntaxx of the language
  formally?}

\kk{If we talk about the language it would be good to give some
  statistics about how many commands (out of the ones that we have
  classified) can be represented with one, two clauses etc. If we have
  a different solution (like python function that given a command and
  its arguments returns the category) then we could talk about how
  many lines of code it took to categorize all the commands that we
  did.}

As described in distributability\ref{}, the abstract notion of a
program that we have is programs with one input and one output
stream. The command developer must be able to express the input
argument of a commmand (or if it is the stdin).

\kk{Question: Do we need to handle all commands (even the ones that
  could have their input split in several different arguments? or can
  we somehow easily make these ones just take their input from one
  place (e.g. by adding a cat before them).}

\kk{Possible solutions for this (if we have already turned commands
  with more that one input to commands with one input): Have a
  positional reference to the argument that is input (or ``stdin'' if
  it is the stdin). Example: the last argument [-1] of grep is its
  input.}

\kk{It seems that even the arguments might depend on flags (e.g. grep
  checks the current directory if asked recursively, or the stdin
  otherwise), so maybe we should again have a sequence of predicates}


\kk{Note: Since a categorization language will probably not be
  complete (especially the one showing the input arguments) there
  should be a backup mechanism, when the language is not expressive
  enough, like a python function that categorizes the command, and
  returns its input argument if it is stateless.}

\subsection{Pure Commands}

Even though stateless commands are a big percentage of all of them,
there are a lot of useful and popular pure commands.

As mentioned in distributability, pure commands can be broken down
into different categories. We might be able to get ideas about this
from this paper:
\url{http://www.cs.toronto.edu/~azadeh/papers/pldi17-ex.pdf} and its
continuation in PLDI 2019.

\kk{Can we find a solution for the commands in coreutils?}

\kk{Important TODO: Find at least one more command that can be
  executed like sort. Does wc work? If there is no pure command that
  can be easily parallelized other than sort, this story is not very
  nice.}



\section{Intermediate Representation/Dataflow graph}
\label{ir}

\kk{Inline listing?}

\kk{Try to be more formal in the section, by using the notation that
  is defined in the nodes paragraph.}

In order to effectively apply optimizations and distribute a shell
script, it first has to be translated to a manipulable
representation. As mentioned in \ref{}, the two main abstractions of
the shell, are files that contain data, and commands that communicate
through these files. We propose a slight variation of the standard
dataflow graph, that is commonly found in stream processing systems,
as the representation of a distributed shell computation. The main
benefit of the dataflow graph model is that it clearly exposes
operator level parallelism \kk{Is this a common term? Is there any
  definitive citation for it?}, as different nodes can be independent
processing workers. In our case each node represents a command, and
each edge represents a file.


\kk{Before that I have to mention that the basic data type $D$ is a
  line of characters. If we move the edges section before the nodes
  that will be fine.}

\paragraph{Nodes - Commands}

Nodes $f$ of the graph represent functions from one (possibly empty)
input stream to an output stream $f : D^* \rightarrow D^*$.  This
representation captures the majority of shell commands. The nodes $f$
are monotonic, meaning that they cannot retract their output. More
formally, if $f(x) = y$, then for any $x'$, there exists $y'$ such
that $f(x.x') = y.y'$, where $.$ is standard concatenation and will be
omitted from now on. An example of a node is the following command
\kk{Does it need ref? Also maybe this example is too trivial and can
  be removed.}:

\begin{lstlisting}[language=sh, float=h, numbers=none, escapeinside={($}{$)}]
 grep ``foo''
\end{lstlisting}

\noindent
This command takes a stream of lines from its standard input and
returns a stream of lines on its standard output. An important
observation though is that commands in the shell might read their
input stream from a sequence of different sources. An illustrative
example is the command \kk{Inline listing}cat which reads its input
from a combination of files and its standard input.

\begin{lstlisting}[language=sh, float=h, numbers=none, escapeinside={($}{$)}]
 cat x - y
\end{lstlisting}

\kk{Should I reference that this example was takes from the cat
  manpage?} The example above reads file \texttt{x} until it
encounters the end of file, then reads from \texttt{stdin}, and then
reads from \texttt{y}. In order to handle this particularity, nodes in
the dataflow graph can have many incoming edges, that are clearly
ordered in a sequence. Note that this comes in contrast with standard
dataflow graph representations where multiple incoming edges either
mean zipping the input to pairs, or arbitraliy interleaving the
different inputs.


\kk{A note about nodes and commands that can write and read from more
  files that the ones in their streams}

Note that nodes can read from more files than just the files of their
stream (or respectively write to more files that their output stream),
however these files are static (e.g. used for configuration or logging
\kk{Would an example here help}) and do not represent streams of data,
and therefore are not considered part of the dataflow graph. Because
of the assumption in section Front End \ref{}, that each file exists
only once in the dataflow graph, we can safely assume that reading and
writing to these static files does not interfere with the distributed
implementation, and doesn't alter the behaviour of the program.

\paragraph{Edges - Files}

\kk{Maybe edges has to be moved before nodes, so that we can define
  the line data type}

Edges in the dataflow graph represent files, the basic data
abstraction of the shell. They are used as communication channels
between nodes in the graph, and as the input or output of the whole
graph. Edges are represented as possibly unbounded streams of type
$D*$, where $D$ represents a line, that is a sequence of characters
followed by \verb|\n| \kk{or EOF?}. They could either refer to a
specific named file in the file system, or just be FIFO pipes that
used for interprocess communication \kk{Are there any other file
  types, such as URLs, ...?}.

\kk{Show an example pipe and its graph}.

In order to expose operator parallelism with the output of a command,
files that don't point to a resource can have an upper bound on the
lines that they can tranfer. The following example illustrates why
this is essential.

\kk{An example of a command that has two outputs and one goes to one
  command, while another output goes to a different one. Maybe only
  show the graph here and not the command itself.}

On the other hand, some forms of data parallelism can be exposed when
knowing the size of the input files. As mentioned in \ref{} some pure
commands (such as cat -n) only need line information to become
stateless, and knowing the size of a file could allow the system to
split it in different chunks that can be processed independently. To
account for that, edges that refer to an input resource contain the
number of lines of the file that they refer to. \kk{At the moment this
  is not supported by our real system. The whole paragraph above can
  go if we decide that it is not a good idea to talk about it now.}

Finally, the edges that don't refer to a resource and do not start
from a node in the graph represent the graph input, while the edges
that don't refer to a resource and do not point to a node in the graph
are its outputs.

\kk{(Maybe) Depending on which of the two paragraphs goes last
  (nodes/edges), I have to explain there that the last incoming edge
  to a node must be unbounded (if it doesn't point to a resource). I
  am actually not sure whether I should include this here.}


\kk{(Maybe) It might be beneficial to mention that many inputs do not
  represent the same thing as many outputs. These two notions are not
  symmetric.}

\paragraph{Merge and Split}

\kk{Mergers and splitters, and how they are implemented using shell
  constructs (cat and head). This subsection might be a better fit in
  some other section, maybe something related to the planner or the
  implementation, as it is a very elegant way to implement splitters
  and mergers while staying completely inside the shell programming
  model.}

It is common in the dataflow graph for a node to have many incoming
edges, or many outgoing edges (when all but the last outgoing edges
are bounded). However some nodes, might read their input from one file
(e.g. stdin). In this case, the distributed implementation needs to
concatenate input files, and split output files. Here is an example:

\kk{Show an example graph! of cat | tr | sort. The command in the
  middle must only take input from one file (maybe just stdin).}

\kk{This transition needs to be stronger} Luckily, the shell offers
primitives for file manipulations that can be used to implement a
merger and a splitter. More concretely a merger can be implemented by
a simple \verb|cat|. Given a set of input files \verb|in1, in2, ...|
and an output file \verb|out|, a merger can be implemented as:

\begin{lstlisting}[language=sh, float=h, numbers=none]
 cat $in1 $in2 ... > $out
\end{lstlisting}

\noindent
On the other hand, given an input file \verb|in| that is a FIFO pipe,
and two output files \verb|out1, out2| the first of which is bounded
to \verb|N| lines, a 2-splitter can be implemented as:

\begin{lstlisting}[language=sh, float=h, numbers=none]
 head -n $N $in > $out1 ; cat $in > $out2
\end{lstlisting}

\noindent
Using the 2-splitter as a building block, a pipe can be split to
arbitrarily many different pipes.

\kk{Do I also need to include a splitter for normal files?}

\section{Front End}

\kk{It might make sense to merge this section with the one below with
  the name: From Shell to IR and back}

As mentioned in \Cref{ir}, in order to generate the distributed
implementation of a shell script, \texttt{Dish} first translates it to
a dataflow graph. However, there are several components in shell
programs that cannot be arbitrarily parallelized without affecting the
program behaviour.

\kk{Give examples of writing to environment variables, producing
  irreversible side effects, the ; and \&\& operators.}

In order to address this issue, we introduce the notion of
distributable AST subtrees, and we design an analysis that given the
shell script Abstract Syntax Tree identifies distributable AST
subtrees and translates each one of them to a dataflow graph. The AST
of the shell script is produced using a POSIX compliant parser
\kk{Reference libdash github}.

\subsection{Distributable Subtrees}

\kk{The goal of this section is to convince the reader ``intuitively''
  that we choose safe parts of a shell program to distribute. There is
  no formal argument, but one should be able to intuitively believe
  us.}

Consider the following example:

\kk{Give an example of a shell script that cannot be distributed
  completely, Possibly involving ; and writing to a variable.}

\kk{Intuitively explain what could be distributed and what not.}

\kk{Our notion of distributable subtrees has two dimensions. First,
  the state of the shell should be an invariant in a distributable
  subtree. We cannot know this completely without evaluating, so we do
  a conservative analysis. The second dimension is that there exists
  at least one component that can be abstracted as a node of the
  dataflow graph. This can be anything that includes a command.}

%% Even though commands are split in categories, and several pipelines
%% are distributable, not all parts of a shell script are stateless and
%% several of them have effects that could affect the behaviour of a
%% program after distribution.

\kk{All of our assumptions and study are based on the POSIX
  standard. Reference?}

\kk{Not checked Assumption. A command doesn't write/read to arbitrary
  files that are not mentioned in its arguments. This is necessary to
  enforce that no two commands concurrently read/write from the same
  file.}

\kk{Not checked Assumption: No signals, traps, and sets at the
  moment. At the moment we don't have a way to handle signals and
  traps.}

\kk{Partially checked Assumption. Only one reader and one writer to
  each file, and no cycles. OLD MORE CONSERVATIVE ASSUMPTION: The same
  file exists once in the substree. This is a conservative assumption
  that ensures that no two commands write or read to the same file
  concurrently. We kind of check it but not completely. We check that
  the expanded evaluated files are not the same, but one could have
  part of a file in a variable etc.}

\kk{Checked assumption: How can we make sure that nodes in the IR are
  deterministic. Do we make some assumption that enforces this? I
  guess that commands that are nodes in the IR are all deterministic?
  That is stateless and pure commands are deterministic.}

\paragraph{analysis}

The analysis is done in a DFS fashion on the AST. Distributable
subtrees grow from the bottom up, until a construct that doesn't
``allow'' them to go through is encountered.

%% Mention all the different constructs that our front end handles, and
%% intuitively mention why it is ``safe'' to add them to the distribution
%% pipeline.

Subtrees don't cross the following:

\begin{itemize}
\item Sequence, And,  or
\item Function definitions
\item Command Substitutions
\item control flow: while, for, if, case: While these constructs can
  be parallelized and there has been research on it, it is orthogonal
  to our work, and can be incorporated as future work.
\end{itemize}

On the other hand: A distributable subtree starts whenever there is a
pipe, or asynchronous list. If the subcomponents of the pipe or
asynchronous list were already graphs, then we connect them to the
bigger graph \kk{explain how we do pipe and asynchronous list}, and if
they were not graphs, we make each subcomponent a node in the subtree,
event though they are not commands. These nodes do not belong to any
category, but still have stdin and stdout, and can be executed
concurrently (as long as they satisfy the checked assumptions above),
since they don't introduce new behaviours, they would still be
executed concurrently when ran in the standard shell, producing
possibly non-deterministic results with different interleavings.

\kk{Explain how we merge two graphs when seeing a pipe}

Whenever we see a subtree of the AST that is a command, we check the
files that it writes to. If any of its arguments is a command
substitution or contains anything that is evaluated\kk{make this more
  precise}, its category is marked as sequential\kk{name?}. Otherwise
we check its its category and its input output arguments (using the
methods described in \Cref{ext}). The command is then initialized as
the node of a singular graph, with the inputs and outputs of the graph
being the ones of the node.


\kk{Maybe mention the recursive case. I am not sure if that should be
  here, or in the distribution planner. In any case, it should be
  after we say that an IR is replaced with a call to the planner with
  the IR saved in a file. The example is a command in a pipeline whose
  first argument is a command substitution with a pipeline. Executing
  the outside dataflow graph, would lead to the command to be executed
  as part of this shell. However its first argument would have a call
  to the planner itself, leading to another distributed implementation
  running in parallel. The rest of the pipeline would wait for this to
  end before it continues. Assuming that the files that both of these
  graphs use are different, and that the above assumptions hold, there
  should be no interference between the two graphs and they should be
  able to run in parallel}

\section{Distribution Planner}

\kk{I think that there is not enough content for the planner section
  and it should probably just be called graph transformations. Then we
  could have a small subsection at the end about how the planner
  applies transformations iteratively starting from the source of the
  graph and pushing them until the end.}

As mentioned in \Cref{ir}, the dataflow graph exposes operator level
parallelism. Different nodes in the graph can be executed
concurrently, communicating through the edges. Operator level
parallelism is already exposed by the shell, by using pipes \texttt{|}
and the background operator \texttt{\&}. Unfortunately, simple
operator level parallelism leaves a lot to be desired, since scaling a
computation requires manually introducing more operators.

In order to adequately utilize the available computational resources,
a distributed implementation should exploit data parallelism. Data
parallelism is based on the observation that some operations produce
the same result when performed independently on subsets of the input
data, merging the results afterwards. Data level parallelism is the
basic optimization that is used by popular distributed processing
systems such as MapReduce\kk{cite} and Apache Spark\kk{cite}.

\subsection{Graph transformations}

In our case, this is achieved by iteratively applying graph
transformations that preserve correctness, while exposing
parallelization opportunities. We illustrate that with a motivating
example; consider the following program and its dataflow graph.

\begin{lstlisting}[language=sh, float=h, numbers=none, escapeinside={($}{$)}]
 grep ``foo'' f1 f2 // Add its graph
\end{lstlisting}

\noindent
In this example, \texttt{grep} is applied to files f1 and f2, and its
result is sent to standard output. However, the result would be the
same if we concatenated the results of \texttt{grep ``foo'' f1} and
\texttt{grep ``foo'' f2}. This intuition can be generalized to all
stateless commands, as they are all essentially ``linear'' with
respect to concatenation and the following equation holds.

\[
\forall f, \mathrm{stateless}(f) \implies f(x.x') = f(x).f(x')
\]

\kk{Complete this story with pure, after we have written the section
  about command categories.}

\subsection{Planner Algorithm}

The \texttt{Dish} planner extends the transformations that were
described above with edge splits in order to allow for the above
transformations to be performed. More precisely, it splits input files
and graph edges into several chunks so that they can be processed in
parallel by the next levels.

\kk{Explain the greedy algorithm that our planner performs. It splits
  input files, and then pushes parallelization through the graph,
  starting from the source nodes.}

\kk{Maybe give an example}



\subsection{Mapping operators to nodes}

\kk{I think we should actually remove this completely.}

Explain the simple algorithm that we used to minimize intra node
communication, that tries to map contiguous parts of the graph to the
same node. We could do this by having an algorithm that minimizes the
number of cuts or something.

We don't try to make a proper planner, as there is a lot of work on
operator placement etc. that we can borrow from.


\subsection{Just in time planning}

\kk{This whole section for just in itme planning, is also relevant for
  the front end. If the front end is executed just in time, it would
  have more information, and maybe it could make a sound analysis.}

Important point: Shell is extremely dynamic. Because of this, a
distributor cannot decide how to distribute a subexpression
statically, by just reading the script, (as in many other systems like
MapReduce, Spark, etc), since most of the information is not there
before the script starts executing. Environment variables,
unexpanded(unevaluated) strings \kk{Make sure that terminology is
  consistent with Greenberg}, could all contain information that is
valuable to make the distribution plan \kk{give an example}.


Future work on this: calling the shell shtepper to only partially
evaluate strings, just expand arguments, and not do any significant
computation. Even better, we could be calling the shtepper just after
having parsed the ast, and decide on maximal distributable subtrees as
late as possible in the process. This would be awesome.

\section{Evaluation}
\label{eval}

\kk{We should make sure to measure the time that it takes for the
  whole process to run (parsing, translating, optimizing, planning)
  together with the scripts. This will probably be negligible, but we
  still have to mention it. }

\kk{Also, we have to have one-two handcrafted examples that have more
  than one pipeline, to show the execution time end to end, as this
  cannot be shown with the one-pipeline examples since they only have
  one graph, so no back and forth between shell and dish.}

\kk{Furthermore it would be great to have one distributed example that
  runs in more than one node (if we have time)}

\kk{Also we have to make sure to measure with and without the final
  cat of output file, so that we show that this is also negligible.}

\kk{We have to make sure that we repeat that the results are the same
  in all tests that we run. Corectness, blah blah ...}

\kk{We should have at least some pipelines that have a bad pure
  command in them.}

At a high level, we are interested in understanding whether \sys can indeed scale pipelines out automatically and correctly.
To achieve this, we use a combination of micro- and macro-benchmarks, most of which were collected from prior literature and online repositories.
Micro-benchmarks~\sx{micro} are one-off, simple one-line pipelines that take a few seconds to write (usually interactively);
  such pipelines are usually composed interactively to solve a task at hand, test a hypothesis, or ``smoke out bugs''~\cite{bentley1986literate}---but, with \sys, applied to very large data sets.
These pipelines are taken from real, often striking~\cite{}, use cases and have the benefit of highlighting stress on a handful of stages, precisely due to their small size.

Three complex macro-benchmarks (\S\ref{macro1}--\ref{macro3}) involve third-party commands and handle realistic workloads by today's standards.
These scripts are about an order of magnitude larger than the micro-benchmarks, but this perceived lack of complexity is deceiving:
  as shown below, if written in conventional languages, these pipelines would correspond to programs on the order of hundreds of lines of code.

Several results are worth highlighting.
First and foremost, programs see a significant speedup, often more $10\times$ over sequential execution  while returning identical results (on very large datasets).
\sys's transformation and planning phase take around XX--XXms, negligible (XX\%) for short-running pipelines and virtually non-existent for long-running ones.
As \sys rewrites to highly-optimized shell primitives rather than using a managed language runtime, it always has a COST~\cite{mcsherryscalability} of 2.
Finally, \sys preserves the productivity of the shell:
  in one case, the equivalent program for a small part of the pipeline requires over a 150 lines of code in combined Java and Bash to run distributed on top of Hadoop;
  in another, the program expressed by the pipeline was the subject of a semester project in a graduate course on distributed systems~\cite{blinded} where student implementations ranged between 1--3\textbf{K} lines of code.

Experiments were run on a \todo{network of five workstations} connected by 1Gbps links: 
 one large machine (\wka) with 512GB of memory and 128 2.1GHz Intel Xeon E5-2683 cores, and 
 \todo{four smaller machines (\wkq), each with 4GB of memory and two 3.33GHz Intel Core Duo E8600 processors}.
For our software setup, we use SMP Debian 4.9.144-3.1 (2019-02-19), GNU Core-utils 8.30-3, GNU Bash 5.0.3(1)-release (x86\_64-pc-linux-gnu), Python 3.7.3, and OCaml 4.05.0.
Generally, no special configuration was made in hardware or software beyond disabling hyper-threading. % ---specifically, the network protocol stack was left unoptimized.
More detailed setup and experiments, including the scripts, is included in the accompanying (anonymized) online repository.


\section{Limitations}

\kk{This is not really a section on its own, but rather part of something else.}

\begin{itemize}
\item Fault Tolerance
\item Distributed Planning: The planner at the moment is naive, and
  there is a huge literature on operator placement of distributed
  dataflow graphs that can be used to improve it.
\item Support of shell constructs: At the moment we have been
  conservative in the shell subsets that we handle, so that we don't
  introduce any unsafe parallelism. The point of this work was not to
  be able to handle a complete distributable subset of the shell, but
  rather a significantly big part of it, to demonstrate that there can
  be performance benefit from it.
\item Support distributing different classes: At the moment we are
  only dealing with stateless and pure commands, that is the ones that
  are most popular but also can offer the biggest benefits from
  parallelizing them. However, it would be interesting to explore the
  distributability of the file system commands, by using a distributed
  file system. THis would also require considerations about
  replication, consistency, and failures.
\item No cycles (multiple commands writing and reading from the same file)
\item Formal proof that the analysis indeed returns distributable
  subtrees. Previously there was no formal semantics for shell, but
  recent work (cite greenberg) allows us to make a formal proof that
  the analysis is indeed sound.
\end{itemize}

\section{Related Work}
\label{related}

At a high-level, techniques for automatically distributing fall under a spectrum that ranges from fully automated (but potentially sub-optimal) distribution to fully manual (but, ideally, optimal) distribution (Fig.~\ref{fig:spectrum}).

\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth]{./figs/dish_spectrum.pdf}
\caption{
  \textbf{Cost of Manual Effort.}
	\sys sits at the automation end of the spectrum, automatically distributing shell pipelines while maintaining their correctness. A more complete picture is presented in the related-work section~\sx{rt}.
}
\label{fig:spectrum}
\end{figure}

% Nice intro: http://homepages.inf.ed.ac.uk/bfranke/Publications/pldi121-tournavitis.pdf
\heading{Automated Parallelization and Distribution}
There is a long history of automated parallelization starting from explicit \ttt{DOALL} and \ttt{DOACROSS} annotations~\cite{par1, par2} and continuing with compilers that attempt to automatically extract parallelism~\cite{padua1993polaris,hall1996maximizing}.
These systems operate at a lower level than \sys (\eg that of instructions or loops instead of pipe boundaries) and typically do not exploit runtime information.

% https://ecommons.cornell.edu/bitstream/handle/1813/6508/85-668.pdf?sequence=1
% The growth of the web led to specialized frameworks for massively distributed computation~\cite{mapreduce:08, spark:10, naiad:13}.
% While they Moreover, these systems take advantage of functional purity;
%   for programs that are not data-intensive processing pipelines (\eg web servers), purely functional code is generally responsible for only a small fraction of the program runtime.
A plethora of systems assist in the construction of distributed software.
Distributed operating systems~\cite{rashid1981accent, walker1983locus, ousterhout1988sprite, mullender1990amoeba, pike1990plan9, rozier1991overview, dorward1997inferno, barak1998mosix, schwarzkopf2013dios, sacha2013osprey} and programming languages~\cite{erlang:96, acute:05, mace:07, cloudhaskell:11}
% simplify many of the problems of distribution and
provide a significant amount of automation. % but require development in a new system or language.  While they take care of all the distribution minutiae, 
However, they involve significant manual effort using the provided abstractions, which are strongly coupled with the underlying operating or runtime system.
\TODO{This has to be broken down into systems and PLs, coz systems are the most relevant/important class.}

\heading{Annotation-based Distribution}
Ignis and Mozart

\heading{DSLs for Distribution}
At the other end of the spectrum, distributed computing frameworks~\cite{mapreduce:08, ciel:11, spark:12, naiad:13, jetstream:14} and domain-specific languages~\cite{alvaro2011consistency, distal:13, meiklejohn2015lasp, psync:16, dartagnan:18}
simplify certain patterns, 
but do not offer the flexibility of a full-fledged environment.
% suffer from a similar lack of generality by making strong assumptions about the nature of the computation---for example, strongly-eventual commutative components that can proceed in parallel.
Developing under these frameworks differs quite significantly from the development of normal (non-distributed) programs.

More recent work focuses on extracting parallelism from domain-specific programming models~\cite{cilk5, streamIt, galois} and interactive parallelization tools~\cite{parascope, ipat}.
These tools simplify the expression of parallelism, but still require programmers to get involved in discovering and exposing parallelism.
%  actively involve the programmer in the detection and mapping of application parallelism, but still demand great effort from the user. 
Moreover, the insights behind these attempts are significantly different from ours, as they extract parallelism statically during compilation instead of dynamically during runtime.
%Despite significant progress~\cite{}, it remains due to its need for complex program analysis and the unknown factors (such as input data range) during compilation.


\section{Conclusion}
\label{discussion}

\kk{We need to cite DGSH}

\begin{acks}
  % Dumping people so that we don't forget
  % 
  This material is based upon work supported by the
  \grantsponsor{GS100000001}{National Science
    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  conclusions or recommendations expressed in this material are those
  of the author and do not necessarily reflect the views of the
  National Science Foundation.
\end{acks}


%% Bibliography
\bibliography{./bib}


%% Appendix
\appendix
\section{Scripts used in the evaluation}

This appendix contains the source code of the scripts used in the evaluation of
the \sys. They are part of the codebase (released as open source with the camera
ready), and are provided here only to aid the reviewers.


\end{document}
