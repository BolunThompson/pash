------------------------------------------------------------------------
Client machine
------------------------------------------------------------------------
User invokes the program with the following command line arguments:
- Raw bash script
- Port on which the client will listen to for a callback by the end worker node that the computation is over and that user can use the results on HDFS
- Name of the file in which the output will be written
- OPTIONAL:
	* -p NUMBER_OF_RUNS -> measure execution performance for NUMBER_OF_RUNS times while discarding first tenth of the measurements due to Java Virtual Machine Warmup

User provides the linear pipeline, thaht is expanded into a graph using parallelization hints (that are hardcoded for now), operators are copied and linked between themselves. Created DAG is then traversed with DFS algorithm. During the traversal the metadata for distributing the work over different workers is created and once that is done the data is sent to coordinator process on a particular cluster. The client then enters into dormant phase, waiting for the remote end DAG worker to send signal that the computation has been done successfully. For now, only one cluster machine is supported due to hardcoding the IP addresses and ports.

------------------------------------------------------------------------
Server machine
------------------------------------------------------------------------

Server machine runs cooridnator process that is in infinite loop waiting accepting new connections by receiving the metadata from various remote clients. It can be run in threaded mode, as well as in mode that creates new process for every new worker. From the information contained in the metadata thread/process is created. Workers are created in order they are received. Only one worker can be the one that will initiate the computation by sending the initial data, and that worker must be started/created last, so the best method is to create workers in reverse order in which they were traversed.
Thread/Process in case it is the initial operator sends the data first, in other case socket is created that waits for input from the workers that execute the operators that were previous in dataflow path. We create pool thread for input channels, and every input channel has its own thread. Each thread writes into its own buffer that is shared between that thread and the thread that cares about particular UNIX shell process. This is done as producer/consumer. The reader thread takes the data from the buffer and pushes the data into the UNIX process, outputting the data whenever it is available on system output of the process. The data is sent in that fashion to next operators. The last operator in this linear pipeline sends the callback signal to client that the computation has finished. 
NOTE:
Along with user data, various system messages are passed through the graph that help the execution be semantically correct.
